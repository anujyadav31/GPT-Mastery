#Developed trigram, character-level, and transformer-based language models from scratch, including custom backpropagation and attention modules. 


#Fine-tuned GPT-style models on custom datasets and optimized efficiency using vectorized multi-head attention and LazyFormer architecture. 
